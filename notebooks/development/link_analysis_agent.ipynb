{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb5dacc-0dab-4c78-8130-d5c4f3855257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/pdf_hunter/agents/link_analysis/tools.py\n",
    "\n",
    "from langchain.tools import tool\n",
    "import whois\n",
    "from whois.parser import PywhoisError\n",
    "import datetime\n",
    "\n",
    "@tool\n",
    "def domain_whois(domain: str) -> str:\n",
    "    \"\"\"\n",
    "    Performs a WHOIS lookup on a root domain to gather registration details.\n",
    "    This is critical for verifying the identity and age of a website.\n",
    "    Use this on the main part of the domain (e.g., 'example.com' from 'http://login.example.com/auth').\n",
    "    \"\"\"\n",
    "    try:\n",
    "        w = whois.whois(domain)\n",
    "        \n",
    "        if w.get('domain_name') is None:\n",
    "            return f\"Error: No WHOIS record found for domain '{domain}'. It may be available for registration or an invalid domain.\"\n",
    "\n",
    "        # Make dates JSON serializable for easier processing by the LLM\n",
    "        for key, value in w.items():\n",
    "            if isinstance(value, datetime.datetime):\n",
    "                w[key] = value.isoformat()\n",
    "            elif isinstance(value, list) and all(isinstance(item, datetime.datetime) for item in value):\n",
    "                w[key] = [item.isoformat() for item in value]\n",
    "\n",
    "        # Return a clean, concise summary of the most important fields\n",
    "        creation_date = w.get('creation_date')\n",
    "        registrar = w.get('registrar')\n",
    "        \n",
    "        summary = (\n",
    "            f\"WHOIS Record for: {w.get('domain_name')}\\n\"\n",
    "            f\"Registrar: {registrar}\\n\"\n",
    "            f\"Creation Date: {creation_date}\\n\"\n",
    "            f\"Expiration Date: {w.get('expiration_date')}\\n\"\n",
    "            f\"Name Servers: {w.get('name_servers')}\"\n",
    "        )\n",
    "        return summary\n",
    "\n",
    "    except PywhoisError as e:\n",
    "        return f\"Error: Could not retrieve WHOIS data for '{domain}'. It might be a subdomain or an invalid domain. Error: {e}\"\n",
    "    except Exception as e:\n",
    "        return f\"An unexpected error occurred during WHOIS lookup for '{domain}': {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454516db-b0af-4b6c-a32d-c0a78380967c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76cb538-9843-4932-8860-3fb48395d6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3791315-0013-4397-8c6c-c5d1e11b0b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "WFI_INVESTIGATOR_SYSTEM_PROMPT = \"\"\"\n",
    "**You are the Web Forensics Investigator (WFI).** Your mission is to conduct a complete, live, interactive forensic investigation of a given URL, from initial reconnaissance to final judgment. You are a skilled and persistent detective, assuming the adversary is using multi-step evasion tactics. Your entire process is governed by a **Core Investigation Loop**.\n",
    "\n",
    "---\n",
    "### **Your Core Investigation Loop (Observe -> Orient -> Decide -> Act)**\n",
    "\n",
    "You will repeat this loop until the mission is complete.\n",
    "\n",
    "**1. OBSERVE: What is the current state?**\n",
    "   *   On your very first turn, your observation is the initial briefing.\n",
    "   *   On all subsequent turns, you MUST use tools to re-observe the state after your last action. Use `browser_take_screenshot`, `browser_evaluate` to get text, and `browser_network_requests` to check for new activity.\n",
    "\n",
    "**2. ORIENT: What does the evidence mean?**\n",
    "   *   Analyze the evidence from your OBSERVE step in the context of your initial briefing (the \"Reason Flagged\").\n",
    "   *   Is this a login page? A redirect page? A page with a download link? A legitimate page?\n",
    "   *   State your hypothesis about the attacker's intent at this specific stage.\n",
    "\n",
    "**3. DECIDE & ACT: What is the next logical step in the pursuit?**\n",
    "   *   Based on your hypothesis, choose the single best tool to move the investigation forward.\n",
    "\n",
    "---\n",
    "### **Tactical Guidance (How to ACT in specific scenarios)**\n",
    "\n",
    "*   **On Initial Navigation (Your First Action):** Your first action is always `browser_navigate` to the URL provided in the briefing. This kicks off the first loop. After this, you MUST perform a full initial observation (screenshot, text, network requests, WHOIS).\n",
    "*   **Verifying Domain Identity:** After you determine the final domain, if it seems suspicious or is trying to impersonate a known brand, you **MUST** use the `domain_whois` tool on its root domain. A recent registration date is a major red flag.\n",
    "*   **Handling Multi-Step Chains:** If the page contains a single, prominent link that seems to be the next step (e.g., a \"Continue\" button), your mission is to **follow it** using `browser_click` or `browser_navigate`.\n",
    "*   **Handling Phishing Forms:** If you encounter a login form, use `browser_fill_form` with generic, non-real credentials and click the login button to see where it leads.\n",
    "\n",
    "---\n",
    "### **Mission Completion**\n",
    "\n",
    "You must conclude the investigation when you reach one of these states:\n",
    "*   **Threat Confirmed:** You have unmasked a phishing page or malicious download. Document the final evidence (including WHOIS data) and state your conclusion.\n",
    "*   **Path Confirmed Benign:** You have followed the chain to a legitimate destination and have verified the domain's identity.\n",
    "*   **Trail Cold:** You have reached a dead end with no further actionable links.\n",
    "\n",
    "**CRITICAL REMINDER:** After every action that changes the page state (`browser_click`, `browser_fill_form`, `browser_navigate`), you MUST restart the **Core Investigation Loop** from the OBSERVE step.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4444b810-284b-4ab7-8dd9-b0106a48f9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dae04a5-49cd-4243-b472-1a88422048f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c162f328-3cad-4468-af67-5b87273f8d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "WFI_ANALYST_SYSTEM_PROMPT = \"\"\"\n",
    "**You are the Web Forensics Analyst.** You are a meticulous and expert synthesizer of evidence. Your sole mission is to review a complete Investigator's Log and produce a final, structured forensic analysis in JSON format.\n",
    "\n",
    "**Your Rules of Engagement:**\n",
    "1.  **Ground Truth is the Log:** You must base your entire report *only* on the provided Investigator Log. Do not infer or hallucinate actions that were not taken.\n",
    "2.  **Synthesize and Extract, Do Not Act:** You do not have tools. Your job is to read, understand, summarize, and extract key pieces of evidence.\n",
    "3.  **Extract Key Data:** Meticulously extract key pieces of evidence from the log: the final URL, the WHOIS record, all screenshot paths, and the investigator's final stated conclusion.\n",
    "4.  **Adhere to the Schema:** Your final output MUST be a single, valid JSON object that strictly conforms to the `AnalystFindings` schema. Do not add any commentary or text outside of the JSON object.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8724f-d198-47b3-bbd4-af3683eac4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb47d709-6b3d-47ef-90eb-34d5ff0e7940",
   "metadata": {},
   "outputs": [],
   "source": [
    "WFI_ANALYST_USER_PROMPT = \"\"\"\n",
    "Conduct a full forensic analysis of the provided investigation log and generate the final `AnalystFindings` JSON report.\n",
    "\n",
    "**1. Initial Briefing (The original mission parameters):**\n",
    "```json\n",
    "{initial_briefing_json}\n",
    "```\n",
    "\n",
    "**2. Full Investigation Log (The \"Detective's Notebook\" from the interactive pursuit):**\n",
    "This is the complete, time-ordered log of every thought, action, and tool output from the field investigator.\n",
    "```json\n",
    "{investigation_log_json}\n",
    "```\n",
    "\n",
    "**Your Mission**:\n",
    "Read and synthesize all of the above evidence into the final `AnalystFindings` JSON object. Your response MUST be only the JSON object.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b91f6c1-7f60-486f-a140-f476c116175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Literal, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22583319-ee79-451e-ac8f-6681c21997a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52ba924-4e62-4844-bad2-cb1d0dd386ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import BaseTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70009d89-8ae0-4bdc-af6f-99b16eee2238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The direct output from the Analyst LLM's synthesis.\n",
    "class AnalystFindings(BaseModel):\n",
    "    \"\"\"The synthesized analysis of the investigation log.\"\"\"\n",
    "    final_url: str = Field(..., description=\"The final destination URL reached by the Investigator.\")\n",
    "    verdict: Literal[\"Benign\", \"Suspicious\", \"Malicious\", \"Inaccessible\"]\n",
    "    confidence: float\n",
    "    summary: str = Field(..., description=\"A concise, executive summary of the investigation, explaining the key findings and the reasoning behind the verdict.\")\n",
    "    detected_threats: List[str] = Field(default_factory=list)\n",
    "    domain_whois_record: Optional[str] = Field(None, description=\"The summary of the WHOIS record for the final root domain, extracted from the investigation log.\")\n",
    "    screenshot_paths: List[str] = Field(default_factory=list, description=\"A list of all screenshot file paths mentioned in the investigation log.\")\n",
    "\n",
    "# The final, assembled forensic report.\n",
    "class URLAnalysisResult(BaseModel):\n",
    "    \"\"\"The final, assembled forensic report, created programmatically by our code.\"\"\"\n",
    "    initial_url: PrioritizedURL\n",
    "    full_investigation_log: List[dict]\n",
    "    analyst_findings: AnalystFindings\n",
    "\n",
    "# The state for our final two-stage pipeline graph.\n",
    "class LinkAnalysisState(TypedDict):\n",
    "    # Inputs\n",
    "    url_task: PrioritizedURL\n",
    "    output_directory: str\n",
    "    session: Any # Holds the live MCP session\n",
    "\n",
    "    # Intermediate result\n",
    "    investigation_log: List[AnyMessage]\n",
    "\n",
    "    # Final output\n",
    "    final_report: URLAnalysisResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b75a97-3b2d-484f-9cda-d436bedfdcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/pdf_hunter/agents/link_analysis/graph.py\n",
    "\n",
    "import asyncio\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "from functools import partial\n",
    "# from langchain_core.messages import SystemMessage, HumanMessage, AnyMessage\n",
    "# from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "# from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "# from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "# from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "# from pdf_hunter.config import link_analysis_llm\n",
    "# from .schemas import LinkAnalysisState, URLAnalysisResult, ScoutReport, RedirectStep\n",
    "# from .prompts import WFI_INVESTigator_SYSTEM_PROMPT, WFI_ANALYST_SYSTEM_PROMPT, WFI_ANALYST_USER_PROMPT\n",
    "# from .tools import domain_whois\n",
    "# from ..visual_analysis.schemas import PrioritizedURL\n",
    "\n",
    "\n",
    "# --- Node 1: Investigator ---\n",
    "async def investigator_node(state: LinkAnalysisState, tools: List[BaseTool]) -> dict:\n",
    "    \"\"\"Performs the full dynamic, interactive investigation.\"\"\"\n",
    "    url_task = state[\"url_task\"]\n",
    "    output_dir = state[\"output_directory\"]\n",
    "    \n",
    "    print(f\"\\n--- [Investigator] Starting full pursuit for {url_task.url} ---\")\n",
    "    \n",
    "    model_with_tools = link_analysis_llm.bind_tools(tools)\n",
    "    \n",
    "    workflow = StateGraph(MessagesState)\n",
    "    workflow.add_node(\"llm\", lambda s: {\"messages\": [model_with_tools.invoke(s[\"messages\"])]})\n",
    "    workflow.add_node(\"tools\", ToolNode(tools))\n",
    "    workflow.add_edge(START, \"llm\")\n",
    "    workflow.add_conditional_edges(\"llm\", tools_condition, {END: END, \"tools\": \"tools\"})\n",
    "    workflow.add_edge(\"tools\", \"llm\")\n",
    "    investigator_graph = workflow.compile()\n",
    "    \n",
    "    initial_prompt = f\"\"\"\n",
    "    Begin your investigation.\n",
    "    **URL:** {url_task.url}\n",
    "    **Reason Flagged:** {url_task.reason}\n",
    "    **Output Directory for Artifacts:** {os.path.abspath(output_dir)}\n",
    "    \"\"\"\n",
    "    initial_state = { \"messages\": [ SystemMessage(content=WFI_INVESTIGATOR_SYSTEM_PROMPT), HumanMessage(content=initial_prompt) ] }\n",
    "    \n",
    "    final_state = await investigator_graph.ainvoke(initial_state)\n",
    "    print(\"\\nðŸ•µï¸â€â™‚ï¸âœ… INVESTIGATION COMPLETE âœ…ðŸ•µï¸â€â™‚ï¸\")\n",
    "    return {\"investigation_log\": final_state[\"messages\"]}\n",
    "    \n",
    "\n",
    "# --- Node 2: Analyst ---\n",
    "async def analyst_node(state: LinkAnalysisState) -> dict:\n",
    "    \"\"\"Synthesizes all evidence and assembles the final report.\"\"\"\n",
    "    url_task = state[\"url_task\"]\n",
    "    investigation_log = state[\"investigation_log\"]\n",
    "\n",
    "    print(\"\\n--- [Analyst] Starting synthesis of all evidence ---\")\n",
    "    \n",
    "    analyst_llm = link_analysis_llm.with_structured_output(AnalystFindings)\n",
    "    analyst_prompt = WFI_ANALYST_USER_PROMPT.format(\n",
    "        initial_briefing_json=url_task.model_dump_json(indent=2),\n",
    "        investigation_log_json=json.dumps([msg.model_dump() for msg in investigation_log], indent=2)\n",
    "    )\n",
    "    analyst_findings = await analyst_llm.ainvoke([\n",
    "        SystemMessage(content=WFI_ANALYST_SYSTEM_PROMPT),\n",
    "        HumanMessage(content=analyst_prompt)\n",
    "    ])\n",
    "\n",
    "    final_report = URLAnalysisResult(\n",
    "        initial_url=url_task,\n",
    "        full_investigation_log=[msg.model_dump() for msg in investigation_log],\n",
    "        analyst_findings=analyst_findings\n",
    "    )\n",
    "    return {\"final_report\": final_report}\n",
    "\n",
    "\n",
    "async def main():\n",
    "    output_dir = \"./mcp_outputs/final_pipeline_test\"\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    url_to_investigate = PrioritizedURL(\n",
    "        url=\"http://hrms.wb.gov.in.hrmspanel.online/\",\n",
    "        priority=1,\n",
    "        reason=\"Flagged as a potential government portal impersonation.\",\n",
    "        page_number=1\n",
    "    )\n",
    "\n",
    "    client = MultiServerMCPClient({\n",
    "        \"playwright\": {\n",
    "            \"command\": \"npx\",\n",
    "            \"args\": [\"@playwright/mcp@latest\", \"--headless\", f\"--output-dir={output_dir}\", \"--save-trace\", \"--isolated\"],\n",
    "            \"transport\": \"stdio\"\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    async with client.session(\"playwright\") as session:\n",
    "        print(\"--- [Orchestrator] Loading all tools for the pipeline ---\")\n",
    "        mcp_tools = await load_mcp_tools(session)\n",
    "        all_tools = mcp_tools + [domain_whois]\n",
    "\n",
    "        # --- Dependency Injection using functools.partial ---\n",
    "        # Create a configured version of the investigator_node that already has the tools.\n",
    "        configured_investigator_node = partial(investigator_node, tools=all_tools)\n",
    "\n",
    "        # Define the main pipeline graph\n",
    "        pipeline = StateGraph(LinkAnalysisState)\n",
    "        pipeline.add_node(\"investigator\", configured_investigator_node)\n",
    "        pipeline.add_node(\"analyst\", analyst_node)\n",
    "        pipeline.add_edge(START, \"investigator\")\n",
    "        pipeline.add_edge(\"investigator\", \"analyst\")\n",
    "        pipeline.add_edge(\"analyst\", END)\n",
    "        link_analysis_graph = pipeline.compile()\n",
    "\n",
    "        # The initial input no longer needs to contain the tools.\n",
    "        initial_input = {\n",
    "            \"url_task\": url_to_investigate,\n",
    "            \"output_directory\": output_dir,\n",
    "            \"session\": session\n",
    "        }\n",
    "        \n",
    "        print(\"\\nðŸš€ Running the full Investigator -> Analyst pipeline...\")\n",
    "        final_state = await link_analysis_graph.ainvoke(initial_input)\n",
    "        \n",
    "        print(\"\\n\\n\" + \"=\"*50)\n",
    "        print(\"ðŸ“Šâœ… FINAL FORENSIC REPORT âœ…ðŸ“Š\")\n",
    "        print(\"=\"*50)\n",
    "        if final_state.get(\"final_report\"):\n",
    "            print(final_state[\"final_report\"].model_dump_json(indent=2))\n",
    "        else:\n",
    "            print(\"Pipeline did not produce a final report.\")\n",
    "        \n",
    "    print(f\"\\nðŸŽ‰ Full pipeline complete! Check {os.path.abspath(output_dir)} for all artifacts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e03e1b-e757-42f9-883f-b217e757aaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb895261-dfd0-4004-8e39-b2a26c0c79ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a53c4f7-63ec-4f59-b986-49344d55c12e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PDF Hunter (venv)",
   "language": "python",
   "name": "pdf-hunter-correct"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
